{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C97fKKFl0XPC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "import shap\n",
        "import re\n",
        "import optuna\n",
        "from scipy.stats import entropy\n",
        "from collections import deque\n",
        "import datetime as dt\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Masking\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBlHDDSuXW2n"
      },
      "outputs": [],
      "source": [
        "# csv загрузка\n",
        "transactions = pd.read_csv(\n",
        "    \"/Users/kabdulasset/Desktop/Hackaton/data/transactions.csv\",\n",
        "    sep=\";\",\n",
        "    quotechar=\"'\",\n",
        "    encoding=\"cp1251\",\n",
        "    skiprows=1,\n",
        "    dtype={\"cst_dim_id\": str}   \n",
        ")\n",
        "behavior = pd.read_csv(\n",
        "    \"/Users/kabdulasset/Desktop/Hackaton/data/behavior.csv\",\n",
        "    sep=\";\",            \n",
        "    encoding=\"cp1251\",   \n",
        "    skiprows=1,\n",
        "    dtype={\"cst_dim_id\": str}   \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJCAtn1AZRp6",
        "outputId": "37f13c6d-18dc-4895-9084-c9977853acb0"
      },
      "outputs": [],
      "source": [
        "# удаление кавычек\n",
        "def remove_quotes(x):\n",
        "    if isinstance(x, str):\n",
        "        return x.replace(\"'\", \"\").strip()\n",
        "    return x\n",
        "\n",
        "behavior[\"transdate\"] = behavior[\"transdate\"].apply(remove_quotes)\n",
        "transactions[\"transdate\"] = transactions[\"transdate\"].apply(remove_quotes)\n",
        "transactions[\"transdatetime\"] = transactions[\"transdatetime\"].apply(remove_quotes)\n",
        "\n",
        "# Преобразуем строки в datetime\n",
        "behavior[\"transdate\"] = pd.to_datetime(behavior[\"transdate\"])\n",
        "transactions[\"transdate\"] = pd.to_datetime(transactions[\"transdate\"])\n",
        "transactions[\"transdatetime\"] = pd.to_datetime(transactions[\"transdatetime\"])\n",
        "\n",
        "print(\"\\nПропуски в behavioral\")\n",
        "print(behavior.isna().sum())\n",
        "\n",
        "print(\"\\nПропуски в transactions\")\n",
        "print(transactions.isna().sum())\n",
        "\n",
        "print(\"\\nРазмер таблиц:\")\n",
        "print(\"Поведенческие:\", behavior.shape)\n",
        "print(\"Транзакции:\", transactions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UHEgXVN0XPF"
      },
      "source": [
        "## Behavior Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "BUqwpkD4Yi9X",
        "outputId": "36fbc162-2d46-4ec2-d5bc-af8405e9bca7"
      },
      "outputs": [],
      "source": [
        "behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAr-2GtaZrp2",
        "outputId": "1f2b594f-ac03-4a2e-e2f0-fc4e653d27de"
      },
      "outputs": [],
      "source": [
        "behavior_rows_with_nan = behavior[behavior.isnull().any(axis=1)]\n",
        "behavior_rows_with_nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZywhouuZvJi"
      },
      "outputs": [],
      "source": [
        "# список столбцов, в которых нужно заменить NaN на 0\n",
        "columns_to_fill = [\"freq_change_7d_vs_mean\", \"logins_7d_over_30d_ratio\"]\n",
        "behavior[columns_to_fill] = behavior[columns_to_fill].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izniokqcbKdC",
        "outputId": "8a72968c-ad38-43e1-94de-90a68008083e"
      },
      "outputs": [],
      "source": [
        "behavior['cst_dim_id'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hekzw1MM0XPE"
      },
      "source": [
        "## Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lGhUpbfbYkKq",
        "outputId": "3663ffaa-b45a-46b1-fa44-baf6d131f813"
      },
      "outputs": [],
      "source": [
        "transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyOK-Uc1Z_uB",
        "outputId": "e535b00e-ee5f-4068-8c6d-3b0a3b837294"
      },
      "outputs": [],
      "source": [
        "# уникальные клиенты в транзакциях\n",
        "transactions['cst_dim_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fMfj8iTyafYj",
        "outputId": "389ac8cd-b4b3-4b9f-afb8-6d8089b0c949"
      },
      "outputs": [],
      "source": [
        "# fraudulent транзакции\n",
        "fraud_tr = transactions[transactions['target'] == 1]\n",
        "fraud_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHe1yMR5aHc5",
        "outputId": "04173eff-37c9-429a-966e-d82a5e75e632"
      },
      "outputs": [],
      "source": [
        "# уникальные клиенты c fraudulent транзакциями\n",
        "transactions[transactions['target'] == 1]['cst_dim_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JHaK1sWoamCg",
        "outputId": "ce1a0cde-be0c-42b7-c5c8-33c927231831"
      },
      "outputs": [],
      "source": [
        "frauds_per_client = (\n",
        "    transactions[transactions['target'] == 1]\n",
        "    .groupby('cst_dim_id')\n",
        "    .size()\n",
        "    .reset_index(name='fraud_count')\n",
        ")\n",
        "frauds_per_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4vzv-Ylza-uP",
        "outputId": "d36b600e-7ac6-4e95-9b75-788be45c7d20"
      },
      "outputs": [],
      "source": [
        "frauds_per_client[frauds_per_client['fraud_count']>1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EPVQqWO0XPG"
      },
      "source": [
        "## Merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ZyI1nWpZ0XPG",
        "outputId": "c2b15ead-adf9-4910-b737-69643bd0c8ad"
      },
      "outputs": [],
      "source": [
        "merged = pd.merge(\n",
        "    transactions,\n",
        "    behavior,\n",
        "    how=\"left\",\n",
        "    on=[\"transdate\", \"cst_dim_id\"],\n",
        "    indicator=True\n",
        ")\n",
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "dV4y1NIc0XPG",
        "outputId": "4c9d3128-ab41-4216-9c5b-ea863227956d"
      },
      "outputs": [],
      "source": [
        "merged[\"_merge\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfsKKOYm0XPG",
        "outputId": "fde6e679-95f7-4b9f-ceb2-e3ed859240e2"
      },
      "outputs": [],
      "source": [
        "merged.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4FlZElX0XPG"
      },
      "outputs": [],
      "source": [
        "merged = merged[merged['_merge']=='both']\n",
        "\n",
        "merged = merged.drop(columns = '_merge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlgexD_y0XPH"
      },
      "source": [
        "## Keep only the first fraud transaction and drop the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FFy91J10XPH"
      },
      "outputs": [],
      "source": [
        "merged = merged.sort_values([\"cst_dim_id\", \"transdatetime\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "2YS2vF3UcHZb",
        "outputId": "7d587b84-5542-4bdf-d971-adb8da99444d"
      },
      "outputs": [],
      "source": [
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKtb3nbd0XPH"
      },
      "outputs": [],
      "source": [
        "first_fraud_idx = (\n",
        "    merged[merged[\"target\"] == 1]\n",
        "    .groupby(\"cst_dim_id\")[\"transdatetime\"]\n",
        "    .idxmin()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouo3fHim0XPH"
      },
      "outputs": [],
      "source": [
        "cleaned_data = merged[\n",
        "    (merged[\"target\"] == 0) |\n",
        "    (merged.index.isin(first_fraud_idx))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__MidCky0XPH",
        "outputId": "f5f49714-a848-4a45-f2e8-3e4d344c1bac"
      },
      "outputs": [],
      "source": [
        "print(\"Before:\", len(merged))\n",
        "print(\"After:\", len(cleaned_data))\n",
        "print(\"Dropped:\", len(merged) - len(cleaned_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u5eonwN0XPH"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FaudmJa0XPH"
      },
      "outputs": [],
      "source": [
        "cleaned_data = cleaned_data[cleaned_data['amount']>0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53j7IKzv0XPH"
      },
      "outputs": [],
      "source": [
        "cleaned_data = cleaned_data[cleaned_data['cst_dim_id'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "uVEpKQJR0XPH",
        "outputId": "7ed8c7d9-5945-49c6-8630-980afa4ef115"
      },
      "outputs": [],
      "source": [
        "cleaned_data.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLX18Tpd0XPH"
      },
      "outputs": [],
      "source": [
        "# 1) Categorical columns → \"Unknown\"\n",
        "cleaned_data[\"last_phone_model_categorical\"] = (\n",
        "    cleaned_data[\"last_phone_model_categorical\"].fillna(\"Unknown\")\n",
        ")\n",
        "\n",
        "cleaned_data[\"last_os_categorical\"] = (\n",
        "    cleaned_data[\"last_os_categorical\"].fillna(\"Unknown\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "DEL2vwZ_0XPI",
        "outputId": "4e22b3ea-fb7e-472d-bee9-7dd7358546b1"
      },
      "outputs": [],
      "source": [
        "cleaned_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk47LEMF0XPI"
      },
      "source": [
        "## Calculating values for 11 фев"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yYEa35c0XPI"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"login_frequency_30d\"] = pd.to_numeric(\n",
        "    cleaned_data[\"login_frequency_30d\"],\n",
        "    errors=\"coerce\"     # make non-numeric values become NaN\n",
        ")\n",
        "\n",
        "# Replace NaN values with computed value\n",
        "cleaned_data[\"login_frequency_30d\"] = cleaned_data[\"login_frequency_30d\"].fillna(\n",
        "    cleaned_data[\"logins_last_30_days\"] / 30\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNGxGeTmCrJ3"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"freq_change_7d_vs_mean\"] = pd.to_numeric(\n",
        "    cleaned_data[\"freq_change_7d_vs_mean\"],\n",
        "    errors=\"coerce\"     # make non-numeric values become NaN\n",
        ")\n",
        "\n",
        "# Replace NaN values with computed value\n",
        "cleaned_data[\"freq_change_7d_vs_mean\"] = cleaned_data[\"freq_change_7d_vs_mean\"].fillna(\n",
        "    (cleaned_data[\"login_frequency_7d\"] - cleaned_data[\"login_frequency_30d\"])/cleaned_data[\"login_frequency_30d\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvbD96mICyjE"
      },
      "outputs": [],
      "source": [
        "# Step 1: replace comma decimal separators with dot\n",
        "cleaned_data['var_login_interval_30d'] = (\n",
        "    cleaned_data['var_login_interval_30d']\n",
        "    .astype(str)                   # ensure strings\n",
        "    .str.replace(',', '.', regex=False)  # convert 1,15E+11 → 1.15E+11\n",
        ")\n",
        "\n",
        "# Step 2: convert to numeric\n",
        "cleaned_data['var_login_interval_30d'] = pd.to_numeric(\n",
        "    cleaned_data['var_login_interval_30d'],\n",
        "    errors='coerce'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "FNd7C7LL0XPI",
        "outputId": "09bac2bc-6bb4-4f0e-80c1-076c86a1dd7e"
      },
      "outputs": [],
      "source": [
        "cleaned_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OS Cleaning and Grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpG0C4Eg6HR_"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"last_os_categorical\"] = (\n",
        "    cleaned_data[\"last_os_categorical\"].fillna(\"Unknown\")\n",
        ")\n",
        "\n",
        "def clean_os_category(x):\n",
        "    if isinstance(x, str):\n",
        "        if x.startswith(\"Android/\"):\n",
        "            return x\n",
        "        if x.startswith(\"iOS/\"):\n",
        "            return x\n",
        "    return \"Unknown\"\n",
        "\n",
        "cleaned_data[\"last_os_categorical\"] = cleaned_data[\"last_os_categorical\"].apply(clean_os_category)\n",
        "\n",
        "\n",
        "def group_os(os_string):\n",
        "    if pd.isna(os_string):\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Normalize input\n",
        "    s = str(os_string).strip()\n",
        "\n",
        "    # Handle Unknown\n",
        "    if s.lower() == \"unknown\":\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Split into platform + version\n",
        "    parts = s.split('/')\n",
        "    if len(parts) < 2:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    platform = parts[0]  # \"iOS\" or \"Android\"\n",
        "    version_raw = parts[1]  # e.g. \"18.6.1\" or \"14\"\n",
        "\n",
        "    # Extract major version\n",
        "    major = version_raw.split('.')[0]\n",
        "\n",
        "    # Clean edge cases (e.g., \"26.0\" for iOS)\n",
        "    try:\n",
        "        major_int = int(major)\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    return f\"{platform}_{major_int}\"\n",
        "\n",
        "\n",
        "cleaned_data[\"last_os_categorical\"] = cleaned_data[\"last_os_categorical\"].apply(group_os)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phone Models Cleaning and Grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEBqoHrl6KPq"
      },
      "outputs": [],
      "source": [
        "# Categorical columns → \"Unknown\"\n",
        "cleaned_data[\"last_phone_model_categorical\"] = (\n",
        "    cleaned_data[\"last_phone_model_categorical\"].fillna(\"Unknown\")\n",
        ")\n",
        "\n",
        "def normalize_device_string(s):\n",
        "    if pd.isna(s):\n",
        "        return s\n",
        "\n",
        "    s = str(s).strip()\n",
        "\n",
        "    # 1. Remove known junk prefixes like \"implyForteApp 1.0 \"\n",
        "    s = re.sub(r'^implyForteApp\\s*\\d+(\\.\\d+)?\\s*', '', s, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Lowercase for uniformity\n",
        "    s = s.lower()\n",
        "\n",
        "    # 3. Replace commas in iPhone versions (iphone14,2 → iphone14_2)\n",
        "    s = s.replace(\",\", \"_\")\n",
        "\n",
        "    # 4. Replace hyphens and spaces with underscores\n",
        "    s = re.sub(r'[\\s\\-]+', '_', s)\n",
        "\n",
        "    # 5. Remove multiple underscores\n",
        "    s = re.sub(r'_+', '_', s)\n",
        "\n",
        "    # 6. Strip leading/trailing underscores\n",
        "    s = s.strip('_')\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "# Apply to your columns\n",
        "cleaned_data[\"last_phone_model_categorical\"] = cleaned_data[\"last_phone_model_categorical\"].apply(normalize_device_string)\n",
        "\n",
        "mask = cleaned_data[\"last_phone_model_categorical\"] == \"x86_64\"\n",
        "\n",
        "cleaned_data.loc[mask, \"last_phone_model_categorical\"] = \"Other\"\n",
        "\n",
        "cleaned_data.loc[mask, \"last_os_categorical\"] = \"Other\"\n",
        "\n",
        "\n",
        "def get_brand(model):\n",
        "    m = model.lower()\n",
        "\n",
        "    if m.startswith(\"iphone\"):\n",
        "        return \"Apple\"\n",
        "    if m.startswith(\"samsung\"):\n",
        "        return \"Samsung\"\n",
        "    if m.startswith(\"xiaomi\") or m.startswith(\"redmi\") or m.startswith(\"poco\"):\n",
        "        return \"Xiaomi\"\n",
        "    if m.startswith(\"oppo\"):\n",
        "        return \"Oppo\"\n",
        "    if m.startswith(\"vivo\"):\n",
        "        return \"Vivo\"\n",
        "    if m.startswith(\"huawei\") or m.startswith(\"honor\"):\n",
        "        return \"Huawei_Honor\"\n",
        "    if m.startswith(\"realme\"):\n",
        "        return \"Realme\"\n",
        "    if m.startswith(\"tecno\"):\n",
        "        return \"Tecno\"\n",
        "    if m.startswith(\"google\") or \"pixel\" in m:\n",
        "        return \"Google\"\n",
        "    if m.startswith(\"motorola\") or \"moto\" in m:\n",
        "        return \"Motorola\"\n",
        "    if m.startswith(\"meizu\"):\n",
        "        return \"Meizu\"\n",
        "    if m in [\"unknown\", \"none\", \"null\"]:\n",
        "        return \"Unknown\"\n",
        "    if m in [\"x86_64\", \"amd64\", \"arm64-v8a\"]:\n",
        "        return \"Other\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "def get_iphone_family(model):\n",
        "    # iphone14_5 → iphone, version = 14\n",
        "    match = re.match(r\"iphone(\\d+)\", model.lower())\n",
        "    if match:\n",
        "        return f\"iPhone_{match.group(1)}\"\n",
        "    return \"iPhone_Other\"\n",
        "\n",
        "def get_samsung_family(model):\n",
        "    m = model.lower()\n",
        "    if \"sm_s\" in m:\n",
        "        return \"Samsung_S\"\n",
        "    if \"sm_a\" in m:\n",
        "        return \"Samsung_A\"\n",
        "    if \"sm_j\" in m:\n",
        "        return \"Samsung_J\"\n",
        "    if \"sm_f7\" in m:\n",
        "        return \"Samsung_Z_Flip\"\n",
        "    if \"sm_f9\" in m:\n",
        "        return \"Samsung_Z_Fold\"\n",
        "    if \"sm_g\" in m:\n",
        "        return \"Samsung_G\"\n",
        "    return \"Samsung_Other\"\n",
        "\n",
        "\n",
        "def get_xiaomi_family(model):\n",
        "    m = model.lower()\n",
        "    if \"redmi\" in m:\n",
        "        return \"Xiaomi_Redmi\"\n",
        "    if \"poco\" in m:\n",
        "        return \"Xiaomi_Poco\"\n",
        "    # Mi series detection extended:\n",
        "    if re.search(r\"m2\\d{2,}\", m):  # m2101k7ag, m2006c3lg, etc\n",
        "        return \"Xiaomi_Mi\"\n",
        "    if re.search(r\"m\\d{2,}\", m):\n",
        "        return \"Xiaomi_Mi\"\n",
        "    return \"Xiaomi_Other\"\n",
        "\n",
        "\n",
        "def get_oppo_family(model):\n",
        "    if \"cph\" in model.lower():\n",
        "        return \"Oppo_CPH\"\n",
        "    return \"Oppo_Other\"\n",
        "\n",
        "def get_vivo_family(model):\n",
        "    m = model.lower()\n",
        "    if re.search(r\"v\\d{3,4}\", m):\n",
        "        return \"Vivo_V\"\n",
        "    if \"vivo\" in m:\n",
        "        return \"Vivo_V\"\n",
        "    return \"Vivo_Other\"\n",
        "\n",
        "def get_huawei_family(model):\n",
        "    return \"Huawei_Honor\"\n",
        "\n",
        "\n",
        "def group_phone_model(model):\n",
        "    if pd.isna(model):\n",
        "        return \"Unknown\"\n",
        "\n",
        "    model = model.lower().strip()\n",
        "\n",
        "    brand = get_brand(model)\n",
        "\n",
        "    if brand == \"Apple\":\n",
        "        return get_iphone_family(model)\n",
        "    if brand == \"Samsung\":\n",
        "        return get_samsung_family(model)\n",
        "    if brand == \"Xiaomi\":\n",
        "        return get_xiaomi_family(model)\n",
        "    if brand == \"Oppo\":\n",
        "        return get_oppo_family(model)\n",
        "    if brand == \"Vivo\":\n",
        "        return get_vivo_family(model)\n",
        "    if brand == \"Huawei_Honor\":\n",
        "        return get_huawei_family(model)\n",
        "    if brand in [\"Realme\", \"Tecno\", \"Motorola\", \"Meizu\", \"Google\"]:\n",
        "        return brand\n",
        "    if brand in [\"Unknown\", \"Other\"]:\n",
        "        return brand\n",
        "\n",
        "    return \"Other\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_data[\"last_phone_model_categorical\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLx6kmyQ6d5C"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"last_phone_model_categorical\"] = cleaned_data[\"last_phone_model_categorical\"].apply(group_phone_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "1Z8_2RkC6gnH",
        "outputId": "2ea8aa03-ca05-4387-a53c-8d335c268c36"
      },
      "outputs": [],
      "source": [
        "cleaned_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNrcET4Q0XPI"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaesNXIv0XPI",
        "outputId": "3099232f-edf2-40f7-bc99-99939a081834"
      },
      "outputs": [],
      "source": [
        "cleaned_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJJnVo0v0XPI"
      },
      "source": [
        "## Time Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-rDkXiB0XPJ"
      },
      "outputs": [],
      "source": [
        "cleaned_data['transdatetime'] = pd.to_datetime(cleaned_data['transdatetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q411EzIM0XPJ"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"hour\"] = cleaned_data[\"transdatetime\"].dt.hour\n",
        "cleaned_data[\"day_of_week\"] = cleaned_data[\"transdatetime\"].dt.weekday\n",
        "cleaned_data[\"is_weekend\"] = cleaned_data[\"day_of_week\"].isin([5, 6]).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ieMVQf0XPJ"
      },
      "source": [
        "## Direction / Recipient Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFbeT7pV0XPJ"
      },
      "outputs": [],
      "source": [
        "#    1 = this direction did NOT exist before for this customer\n",
        "#    0 = this direction already existed in the customer's past\n",
        "cleaned_data[\"is_new_direction\"] = (\n",
        "    cleaned_data\n",
        "    .groupby(\"cst_dim_id\")[\"direction\"]\n",
        "    .transform(lambda s: (~s.duplicated()).astype(int))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7bHcUaGt6IE"
      },
      "outputs": [],
      "source": [
        "# Count how often the client used each direction BEFORE the current transaction\n",
        "cleaned_data = cleaned_data.sort_values([\"cst_dim_id\", \"transdatetime\"]).copy()\n",
        "\n",
        "cleaned_data[\"direction_count_before\"] = (\n",
        "    cleaned_data\n",
        "        .groupby([\"cst_dim_id\", \"direction\"])\n",
        "        .cumcount()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MULp8UQb0XPJ"
      },
      "outputs": [],
      "source": [
        "# \"frequent direction\" (>= 2 previous transactions)\n",
        "cleaned_data[\"is_frequent_direction\"] = (cleaned_data[\"direction_count_before\"] >= 2).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yarGGNg30XPJ"
      },
      "outputs": [],
      "source": [
        "cleaned_data[\"had_previous_transactions\"] = (cleaned_data[\"direction_count_before\"] >= 1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOqXNF5JwnrU",
        "outputId": "228371d4-5934-4634-b832-974920c35870"
      },
      "outputs": [],
      "source": [
        "# How many different recipients the client has used\n",
        "# High = unusual, possibly fraud\n",
        "cleaned_data = cleaned_data.sort_values([\"cst_dim_id\", \"transdatetime\"]).copy()\n",
        "\n",
        "def compute_unique_dirs(group):\n",
        "    seen = set()\n",
        "    unique_counts = []\n",
        "\n",
        "    for idx, row in group.iterrows():\n",
        "        unique_counts.append(len(seen))  # BEFORE this tx\n",
        "        seen.add(row[\"direction\"])       # update AFTER\n",
        "\n",
        "    group[\"num_unique_directions\"] = unique_counts\n",
        "    return group\n",
        "\n",
        "result = (\n",
        "    cleaned_data\n",
        "    .groupby(\"cst_dim_id\")\n",
        "    .apply(compute_unique_dirs)\n",
        "    .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "cleaned_data[\"num_unique_directions\"] = result[\"num_unique_directions\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbyK4Kff0XPK"
      },
      "source": [
        "## Client-Level Historical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWb_UYHt18Ma"
      },
      "outputs": [],
      "source": [
        "cleaned_data = cleaned_data.sort_values([\"cst_dim_id\", \"transdatetime\"]).copy()\n",
        "\n",
        "# Shifted amount (so each row sees only previous transactions) ----\n",
        "shifted_amount = cleaned_data.groupby(\"cst_dim_id\")[\"amount\"].shift(1)\n",
        "\n",
        "# Expanding mean/std computed ONLY on shifted values ----\n",
        "expanding_stats = (\n",
        "    shifted_amount\n",
        "        .groupby(cleaned_data[\"cst_dim_id\"])\n",
        "        .expanding()\n",
        "        .agg([\"mean\", \"std\"])\n",
        ")\n",
        "\n",
        "# Remove hierarchical index\n",
        "expanding_stats = expanding_stats.reset_index(level=0, drop=True)\n",
        "\n",
        "cleaned_data[\"amount_mean_before\"] = expanding_stats[\"mean\"].fillna(0)\n",
        "cleaned_data[\"amount_std_before\"]  = expanding_stats[\"std\"].fillna(0)\n",
        "cleaned_data[\"amount_var_before\"]  = cleaned_data[\"amount_std_before\"] ** 2\n",
        "\n",
        "# Z-score ----\n",
        "cleaned_data[\"amount_zscore\"] = (\n",
        "    (cleaned_data[\"amount\"] - cleaned_data[\"amount_mean_before\"]) /\n",
        "    (cleaned_data[\"amount_std_before\"] + 1e-9)\n",
        ")\n",
        "\n",
        "cleaned_data[\"amount_zscore\"] = cleaned_data[\"amount_zscore\"].fillna(0)\n",
        "\n",
        "# Same amount sent before ----\n",
        "cleaned_data[\"same_amount_sent_before\"] = (\n",
        "    cleaned_data.groupby([\"cst_dim_id\", \"amount\"])\n",
        "    .cumcount()\n",
        "    .gt(0)\n",
        "    .astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvUvXu78N-AV"
      },
      "outputs": [],
      "source": [
        "cleaned_data['time_since_last_tx'] = (\n",
        "    cleaned_data.groupby('cst_dim_id')['transdatetime']\n",
        "    .diff()\n",
        "    .dt.total_seconds()\n",
        "    .fillna(0)  # 0 for first transaction\n",
        ")\n",
        "# Optional: Log-transform for skewed distribution\n",
        "cleaned_data['log_time_since_last_tx'] = np.log1p(cleaned_data['time_since_last_tx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSdE1aeN2oB8"
      },
      "outputs": [],
      "source": [
        "cleaned_data['amount_max_before'] = (\n",
        "    cleaned_data.groupby('cst_dim_id')['amount']\n",
        "    .expanding()\n",
        "    .max()\n",
        "    .shift(1)\n",
        "    .reset_index(level=0, drop=True)\n",
        "    .fillna(0)\n",
        ")\n",
        "cleaned_data['amount_min_before'] = (\n",
        "    cleaned_data.groupby('cst_dim_id')['amount']\n",
        "    .expanding()\n",
        "    .min()\n",
        "    .shift(1)\n",
        "    .reset_index(level=0, drop=True)\n",
        "    .fillna(cleaned_data['amount'])  # Use current for first tx\n",
        ")\n",
        "cleaned_data['amount_ratio_to_max'] = cleaned_data['amount'] / (cleaned_data['amount_max_before'] + 1e-9)\n",
        "cleaned_data['amount_ratio_to_min'] = cleaned_data['amount'] / (cleaned_data['amount_min_before'] + 1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79DcLzzi48Ne"
      },
      "outputs": [],
      "source": [
        "# Ratio of recent logins to transaction amount (high logins + low amount might be testing)\n",
        "cleaned_data['logins_to_amount_ratio'] = cleaned_data['logins_last_7_days'] / (cleaned_data['amount'] + 1e-9)\n",
        "\n",
        "# Z-score of login frequency relative to amount z-score\n",
        "cleaned_data['login_amount_interaction_z'] = cleaned_data['zscore_avg_login_interval_7d'] * cleaned_data['amount_zscore']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlunxpid48zv"
      },
      "outputs": [],
      "source": [
        "# Cumulative sum of amounts per customer BEFORE current tx\n",
        "cleaned_data['cum_amount_before'] = (\n",
        "    cleaned_data.groupby('cst_dim_id')['amount']\n",
        "    .expanding()\n",
        "    .sum()\n",
        "    .shift(1)\n",
        "    .reset_index(level=0, drop=True)\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Velocity: Cumulative amount / days since first tx\n",
        "first_tx_date = cleaned_data.groupby('cst_dim_id')['transdatetime'].min().reset_index(name='first_tx')\n",
        "cleaned_data = cleaned_data.merge(first_tx_date, on='cst_dim_id')\n",
        "cleaned_data['days_since_first'] = (cleaned_data['transdatetime'] - cleaned_data['first_tx']).dt.days + 1  # Avoid div by 0\n",
        "cleaned_data['amount_velocity'] = cleaned_data['cum_amount_before'] / cleaned_data['days_since_first']\n",
        "cleaned_data = cleaned_data.drop(columns=['first_tx'])  # Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxtX8RvR5A2k",
        "outputId": "6bb38744-ce3b-478d-9b8f-b98a2cc3cd43"
      },
      "outputs": [],
      "source": [
        "def direction_entropy(group):\n",
        "    seen_dirs = []\n",
        "    entropies = []\n",
        "    for dir in group['direction']:\n",
        "        seen_dirs.append(dir)\n",
        "        dir_counts = pd.Series(seen_dirs).value_counts(normalize=True)\n",
        "        entropies.append(entropy(dir_counts))\n",
        "    group['direction_entropy_before'] = [0] + entropies[:-1]  # Shift to BEFORE current\n",
        "    return group\n",
        "\n",
        "cleaned_data = cleaned_data.groupby('cst_dim_id').apply(direction_entropy).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbxsnbH4sY9"
      },
      "outputs": [],
      "source": [
        "# Must be sorted\n",
        "cleaned_data = cleaned_data.sort_values([\"cst_dim_id\", \"transdatetime\"]).copy()\n",
        "\n",
        "# Preallocate columns\n",
        "cleaned_data[\"tx_count_last_24h\"] = 0\n",
        "cleaned_data[\"tx_count_last_7d\"] = 0\n",
        "\n",
        "# Sliding-window per user\n",
        "for cust_id, group_idx in cleaned_data.groupby(\"cst_dim_id\").groups.items():\n",
        "    idx_list = list(group_idx)\n",
        "\n",
        "    window_24h = deque()\n",
        "    window_7d = deque()\n",
        "\n",
        "    for pos, idx in enumerate(idx_list):\n",
        "        t = cleaned_data.at[idx, \"transdatetime\"]\n",
        "\n",
        "        # Remove outdated from 24h window\n",
        "        while window_24h and t - cleaned_data.at[window_24h[0], \"transdatetime\"] > pd.Timedelta(\"1D\"):\n",
        "            window_24h.popleft()\n",
        "\n",
        "        # Remove outdated from 7d window\n",
        "        while window_7d and t - cleaned_data.at[window_7d[0], \"transdatetime\"] > pd.Timedelta(\"7D\"):\n",
        "            window_7d.popleft()\n",
        "\n",
        "        # Count BEFORE current transaction\n",
        "        cleaned_data.at[idx, \"tx_count_last_24h\"] = len(window_24h)\n",
        "        cleaned_data.at[idx, \"tx_count_last_7d\"]  = len(window_7d)\n",
        "\n",
        "        # Add current to windows AFTER counting\n",
        "        window_24h.append(idx)\n",
        "        window_7d.append(idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ug8-o30XPK"
      },
      "source": [
        "# Clean / Finalize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHVqPNRj0XPL"
      },
      "outputs": [],
      "source": [
        "clients = cleaned_data[\"cst_dim_id\"].unique()\n",
        "train_clients, test_clients = train_test_split(\n",
        "    clients, test_size=0.2, random_state=40\n",
        ")\n",
        "\n",
        "train = cleaned_data[cleaned_data.cst_dim_id.isin(train_clients)]\n",
        "test  = cleaned_data[cleaned_data.cst_dim_id.isin(test_clients)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "rTn5NiO50XPL",
        "outputId": "d1e1566f-5089-4c22-ae83-33a6b9987cfb"
      },
      "outputs": [],
      "source": [
        "train[train['target']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssloGIbv0XPL",
        "outputId": "b30f0f9a-9a09-43df-e917-f3d4762f5db8"
      },
      "outputs": [],
      "source": [
        "train['cst_dim_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQM1vncY0XPL"
      },
      "outputs": [],
      "source": [
        "y_train = train[\"target\"]\n",
        "y_test = test[\"target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "RANDOM_STATE = 40\n",
        "TIME_STEPS   = 10  # look-back window size\n",
        "LATENT_DIM   = 16\n",
        "LSTM_UNITS   = 32\n",
        "EPOCHS       = 20\n",
        "BATCH_SIZE   = 64\n",
        "\n",
        "# ======================================================================\n",
        "# 1. Use only outer TRAIN data to train the autoencoder\n",
        "# ======================================================================\n",
        "ready_data = (\n",
        "    train\n",
        "    .reset_index(drop=True)\n",
        "    .sort_values([\"cst_dim_id\", \"transdatetime\"])\n",
        ")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. Identify client groups in TRAIN\n",
        "# ======================================================================\n",
        "all_clients   = ready_data[\"cst_dim_id\"].unique()\n",
        "fraud_clients = ready_data.loc[ready_data['target'] == 1, 'cst_dim_id'].unique()\n",
        "\n",
        "# Clients that never had fraud in TRAIN\n",
        "normal_only_clients = np.setdiff1d(all_clients, fraud_clients)\n",
        "\n",
        "print(f\"Total clients in TRAIN: {len(all_clients)}\")\n",
        "print(f\"Clients with fraud in TRAIN: {len(fraud_clients)}\")\n",
        "print(f\"Normal-only clients in TRAIN: {len(normal_only_clients)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3. Split TRAIN data into:\n",
        "#    - lstm_train_normals_df: normal-only clients (for training AE)\n",
        "#    - lstm_train_fraud_df: clients with any fraud (for scoring only)\n",
        "# ======================================================================\n",
        "lstm_train_normals_df = (\n",
        "    ready_data\n",
        "    .loc[ready_data.cst_dim_id.isin(normal_only_clients)]\n",
        "    .copy()\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "lstm_train_fraud_df = (\n",
        "    ready_data\n",
        "    .loc[ready_data.cst_dim_id.isin(fraud_clients)]\n",
        "    .copy()\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"LSTM training rows (normal-only clients): {len(lstm_train_normals_df)}\")\n",
        "print(f\"LSTM side rows (clients with fraud in TRAIN): {len(lstm_train_fraud_df)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. Feature definition + preprocessing (numeric + OHE + scaling)\n",
        "# ======================================================================\n",
        "\n",
        "# Columns that should NOT be used as features\n",
        "DROP_COLS = [\"cst_dim_id\", \"transdate\", \"transdatetime\", \"docno\", \"direction\", \"target\"]\n",
        "CAT_COLS  = [\"last_phone_model_categorical\", \"last_os_categorical\"]\n",
        "\n",
        "FEATURE_EXCLUDE = set(DROP_COLS + CAT_COLS)\n",
        "\n",
        "# Numeric columns: all else except DROP_COLS and CAT_COLS\n",
        "NUM_COLS = [c for c in lstm_train_normals_df.columns if c not in FEATURE_EXCLUDE]\n",
        "\n",
        "print(\"Numeric feature columns:\", NUM_COLS)\n",
        "print(\"Categorical (to OHE) columns:\", CAT_COLS)\n",
        "\n",
        "# We'll also need a copy of TEST for feature processing\n",
        "test_df_for_features = test.copy()\n",
        "\n",
        "# 4.1. Numeric: enforce numeric type & impute NaNs\n",
        "for col in NUM_COLS:\n",
        "    lstm_train_normals_df[col] = pd.to_numeric(lstm_train_normals_df[col], errors='coerce').fillna(0)\n",
        "    lstm_train_fraud_df[col]   = pd.to_numeric(lstm_train_fraud_df[col],   errors='coerce').fillna(0)\n",
        "    test_df_for_features[col]  = pd.to_numeric(test_df_for_features[col],  errors='coerce').fillna(0)\n",
        "\n",
        "# 4.2. One-Hot Encoding on categorical features\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "X_train_norm_cat = ohe.fit_transform(lstm_train_normals_df[CAT_COLS])\n",
        "X_train_fraud_cat = ohe.transform(lstm_train_fraud_df[CAT_COLS])\n",
        "X_test_cat = ohe.transform(test_df_for_features[CAT_COLS])\n",
        "\n",
        "ohe_cols = ohe.get_feature_names_out(CAT_COLS)\n",
        "\n",
        "X_train_norm_cat_df = pd.DataFrame(X_train_norm_cat,  columns=ohe_cols, index=lstm_train_normals_df.index)\n",
        "X_train_fraud_cat_df = pd.DataFrame(X_train_fraud_cat, columns=ohe_cols, index=lstm_train_fraud_df.index)\n",
        "X_test_cat_df  = pd.DataFrame(X_test_cat,            columns=ohe_cols, index=test_df_for_features.index)\n",
        "\n",
        "# 4.3. Scale numeric columns only (MinMaxScaler)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train_norm_num_scaled  = scaler.fit_transform(lstm_train_normals_df[NUM_COLS])\n",
        "X_train_fraud_num_scaled = scaler.transform(lstm_train_fraud_df[NUM_COLS])\n",
        "X_test_num_scaled        = scaler.transform(test_df_for_features[NUM_COLS])\n",
        "\n",
        "X_train_norm_num_df  = pd.DataFrame(X_train_norm_num_scaled,  columns=NUM_COLS, index=lstm_train_normals_df.index)\n",
        "X_train_fraud_num_df = pd.DataFrame(X_train_fraud_num_scaled, columns=NUM_COLS, index=lstm_train_fraud_df.index)\n",
        "X_test_num_df        = pd.DataFrame(X_test_num_scaled,        columns=NUM_COLS, index=test_df_for_features.index)\n",
        "\n",
        "# 4.4. Final processed feature matrices\n",
        "X_train_norm_processed  = pd.concat([X_train_norm_num_df,  X_train_norm_cat_df],  axis=1)\n",
        "X_train_fraud_processed = pd.concat([X_train_fraud_num_df, X_train_fraud_cat_df], axis=1)\n",
        "X_test_processed        = pd.concat([X_test_num_df,        X_test_cat_df],        axis=1)\n",
        "\n",
        "N_FEATURES = X_train_norm_processed.shape[1]\n",
        "print(f\"Total features (N_FEATURES) for LSTM input: {N_FEATURES}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 5. Sequence creation (per client, padded to TIME_STEPS)\n",
        "# ======================================================================\n",
        "def create_sequences(X_df, df_original, time_steps):\n",
        "    \"\"\"\n",
        "    Creates sequences per client (sliding window up to 'time_steps'),\n",
        "    pads from the front with zeros if shorter than time_steps.\n",
        "\n",
        "    Returns:\n",
        "        X_seq: np.array of shape (N_sequences, time_steps, N_features)\n",
        "        indices: list of original row indices corresponding to the last step\n",
        "    \"\"\"\n",
        "    X_seq = []\n",
        "    indices = []\n",
        "\n",
        "    df_scaled = X_df.copy()\n",
        "    n_features = df_scaled.shape[1]\n",
        "\n",
        "    for cst_id, group in df_original.groupby('cst_dim_id'):\n",
        "        client_indices = group.index\n",
        "        client_data = df_scaled.loc[client_indices].values\n",
        "        n_transactions = len(client_data)\n",
        "\n",
        "        for i in range(n_transactions):\n",
        "            start_index = max(0, i - time_steps + 1)\n",
        "            sequence = client_data[start_index:i+1]\n",
        "\n",
        "            # Pad with zeros if shorter than time_steps\n",
        "            if len(sequence) < time_steps:\n",
        "                padding_needed = time_steps - len(sequence)\n",
        "                padding = np.zeros((padding_needed, n_features))\n",
        "                sequence = np.vstack((padding, sequence))\n",
        "\n",
        "            X_seq.append(sequence)\n",
        "            indices.append(client_indices[i])\n",
        "\n",
        "    return np.array(X_seq), indices\n",
        "\n",
        "# ======================================================================\n",
        "# 6. Build sequences for:\n",
        "#    - normal-only TRAIN (for training the autoencoder)\n",
        "#    - fraud-client TRAIN (for scoring)\n",
        "#    - full TEST (for scoring)\n",
        "# ======================================================================\n",
        "X_train_norm_seq,  train_norm_indices  = create_sequences(X_train_norm_processed,  lstm_train_normals_df, TIME_STEPS)\n",
        "X_train_fraud_seq, train_fraud_indices = create_sequences(X_train_fraud_processed, lstm_train_fraud_df,   TIME_STEPS)\n",
        "X_test_seq,        test_indices        = create_sequences(X_test_processed,        test,                  TIME_STEPS)\n",
        "\n",
        "y_train_norm_seq = X_train_norm_seq\n",
        "\n",
        "print(\"\\nSequence shapes:\")\n",
        "print(f\"TRAIN normal sequences: {X_train_norm_seq.shape}\")\n",
        "print(f\"TRAIN fraud sequences:  {X_train_fraud_seq.shape}\")\n",
        "print(f\"TEST sequences:         {X_test_seq.shape}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 7. Define & train the LSTM Autoencoder (on normal-only sequences)\n",
        "# ======================================================================\n",
        "model = Sequential([\n",
        "    Masking(mask_value=0.0, input_shape=(TIME_STEPS, N_FEATURES)),\n",
        "\n",
        "    # Encoder\n",
        "    LSTM(LSTM_UNITS, activation='relu', return_sequences=True),\n",
        "    LSTM(LATENT_DIM, activation='relu', return_sequences=False),\n",
        "\n",
        "    # Repeat latent vector\n",
        "    RepeatVector(TIME_STEPS),\n",
        "\n",
        "    # Decoder\n",
        "    LSTM(LATENT_DIM, activation='relu', return_sequences=True),\n",
        "    LSTM(LSTM_UNITS, activation='relu', return_sequences=True),\n",
        "\n",
        "    # Output: reconstruct features at each time step\n",
        "    TimeDistributed(Dense(N_FEATURES))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "print(\"\\nStarting AE training on normal-only clients...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_norm_seq, y_train_norm_seq,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    shuffle=False,   # keep temporal ordering\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"AE training finished.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 8. Use trained AE to reconstruct & compute anomaly scores\n",
        "# ======================================================================\n",
        "X_train_norm_pred  = model.predict(X_train_norm_seq,  verbose=0)\n",
        "X_train_fraud_pred = model.predict(X_train_fraud_seq, verbose=0)\n",
        "X_test_pred        = model.predict(X_test_seq,        verbose=0)\n",
        "\n",
        "def calculate_anomaly_scores(X_true_seq, X_pred_seq, indices, original_df):\n",
        "    \"\"\"\n",
        "    Calculates MAE on the last time step and merges back to original_df.\n",
        "    \"\"\"\n",
        "    X_true_last = X_true_seq[:, TIME_STEPS - 1, :]\n",
        "    X_pred_last = X_pred_seq[:, TIME_STEPS - 1, :]\n",
        "\n",
        "    mae = np.mean(np.abs(X_true_last - X_pred_last), axis=1)\n",
        "\n",
        "    scores_df = pd.DataFrame({'anomaly_score_lstm': mae}, index=indices)\n",
        "\n",
        "    final_df = original_df.merge(\n",
        "        scores_df,\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "    return final_df\n",
        "\n",
        "train_norm_with_scores  = calculate_anomaly_scores(X_train_norm_seq,  X_train_norm_pred,  train_norm_indices,  lstm_train_normals_df)\n",
        "train_fraud_with_scores = calculate_anomaly_scores(X_train_fraud_seq, X_train_fraud_pred, train_fraud_indices, lstm_train_fraud_df)\n",
        "test_with_scores        = calculate_anomaly_scores(X_test_seq,        X_test_pred,        test_indices,        test)\n",
        "\n",
        "# Combine normal+fraud TRAIN\n",
        "full_train_with_scores = pd.concat(\n",
        "    [train_norm_with_scores, train_fraud_with_scores],\n",
        "    axis=0,\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "print(\"\\n--- Anomaly score shapes ---\")\n",
        "print(\"Normal TRAIN with scores:\", train_norm_with_scores.shape)\n",
        "print(\"Fraud TRAIN with scores:\",  train_fraud_with_scores.shape)\n",
        "print(\"FULL TRAIN with scores:\",   full_train_with_scores.shape)\n",
        "print(\"TEST with scores:\",         test_with_scores.shape)\n",
        "\n",
        "print(\"\\nExample TEST rows with anomaly score:\")\n",
        "print(test_with_scores[['cst_dim_id', 'transdatetime', 'amount', 'target', 'anomaly_score_lstm']].head())\n",
        "\n",
        "# ======================================================================\n",
        "# 9. Quick sanity check: distributions\n",
        "# ======================================================================\n",
        "plt.figure()\n",
        "full_train_with_scores['anomaly_score_lstm'].hist(bins=50)\n",
        "plt.title(\"FULL TRAIN anomaly_score_lstm\")\n",
        "\n",
        "plt.figure()\n",
        "test_with_scores['anomaly_score_lstm'].hist(bins=50)\n",
        "plt.title(\"TEST anomaly_score_lstm\")\n",
        "\n",
        "fraud_test_dist = test_with_scores[test_with_scores['target'] == 1]\n",
        "plt.figure()\n",
        "fraud_test_dist['anomaly_score_lstm'].hist(bins=50)\n",
        "plt.title(\"Fraud (TEST) anomaly_score_lstm\")\n",
        "\n",
        "print(\"\\nMeans:\")\n",
        "print(\"FULL TRAIN mean:\", full_train_with_scores['anomaly_score_lstm'].mean())\n",
        "print(\"TEST mean:\",       test_with_scores['anomaly_score_lstm'].mean())\n",
        "\n",
        "print(\"\\nMedians:\")\n",
        "print(\"FULL TRAIN median:\", full_train_with_scores['anomaly_score_lstm'].median())\n",
        "print(\"TEST median:\",       test_with_scores['anomaly_score_lstm'].median())\n",
        "\n",
        "# ======================================================================\n",
        "# 10. Supervised model: build TRAIN/VAL from TRAIN only\n",
        "# ======================================================================\n",
        "\n",
        "# Define which columns to drop as features for supervised model\n",
        "SUPER_DROP_COLS = [\"cst_dim_id\", \"target\", \"transdate\", \"transdatetime\", \"docno\", \"direction\"]\n",
        "\n",
        "X_train_final = full_train_with_scores.drop(columns=[c for c in SUPER_DROP_COLS if c in full_train_with_scores.columns])\n",
        "y_train_final = full_train_with_scores[\"target\"]\n",
        "\n",
        "X_test_final = test_with_scores.drop(columns=[c for c in SUPER_DROP_COLS if c in test_with_scores.columns])\n",
        "y_test_final = test_with_scores[\"target\"]\n",
        "\n",
        "print(\"\\nSupervised feature shapes BEFORE split:\")\n",
        "print(\"X_train_final:\", X_train_final.shape)\n",
        "print(\"y_train_final:\", y_train_final.shape)\n",
        "print(\"X_test_final:\",  X_test_final.shape)\n",
        "print(\"y_test_final:\",  y_test_final.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Clean numeric columns BEFORE encoding\n",
        "cat_cols = [\"last_phone_model_categorical\", \"last_os_categorical\"]\n",
        "num_cols = [c for c in X_train_final.columns if c not in cat_cols]\n",
        "\n",
        "# Convert to numeric (non-numeric -> NaN)\n",
        "X_train_final[num_cols] = X_train_final[num_cols].apply(\n",
        "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
        ")\n",
        "X_test_final[num_cols] = X_test_final[num_cols].apply(\n",
        "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "X_train_ohe = ohe.fit_transform(X_train_final[cat_cols])\n",
        "X_test_ohe  = ohe.transform(X_test_final[cat_cols])\n",
        "\n",
        "ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
        "\n",
        "X_train_ohe = pd.DataFrame(X_train_ohe, columns=ohe_cols, index=X_train_final.index)\n",
        "X_test_ohe  = pd.DataFrame(X_test_ohe,  columns=ohe_cols, index=X_test_final.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_final = pd.concat([X_train_final[num_cols], X_train_ohe], axis=1)\n",
        "X_test_final  = pd.concat([X_test_final[num_cols],  X_test_ohe],  axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neg = (y_train_final == 0).sum()\n",
        "pos = (y_train_final == 1).sum()\n",
        "scale_pos_weight = neg / max(pos, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X_train_final.reset_index(drop=True)\n",
        "y = y_train_final.reset_index(drop=True)\n",
        "groups = train[\"cst_dim_id\"].reset_index(drop=True)\n",
        "\n",
        "cv = StratifiedGroupKFold(\n",
        "    n_splits=5,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Take the *first* fold as your train/validation split\n",
        "for tr_idx, val_idx in cv.split(X, y, groups):\n",
        "    X_tr  = X.iloc[tr_idx]\n",
        "    y_tr  = y.iloc[tr_idx]\n",
        "    X_val = X.iloc[val_idx]\n",
        "    y_val = y.iloc[val_idx]\n",
        "    break  # only use the first split\n",
        "\n",
        "print(\"\\nAfter StratifiedGroupKFold split:\")\n",
        "print(\"X_tr:\", X_tr.shape, \"  y_tr:\", y_tr.shape)\n",
        "print(\"X_val:\", X_val.shape, \" y_val:\", y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgvu3wfb0XPL"
      },
      "source": [
        "## Optuna Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e84y2EWG0XPL"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 15.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
        "\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"n_jobs\": -1,\n",
        "        \"random_state\": 40,\n",
        "    }\n",
        "\n",
        "    fold_aucs = []\n",
        "\n",
        "    for train_idx, val_idx in cv.split(X, y, groups):\n",
        "        # IMPORTANT: use .iloc for row selection on DataFrames/Series\n",
        "        X_tr = X.iloc[train_idx]\n",
        "        X_val = X.iloc[val_idx]\n",
        "        y_tr = y.iloc[train_idx]\n",
        "        y_val = y.iloc[val_idx]\n",
        "\n",
        "        # Handle class imbalance per fold\n",
        "        neg = (y_tr == 0).sum()\n",
        "        pos = (y_tr == 1).sum()\n",
        "        params[\"scale_pos_weight\"] = neg / max(pos, 1)\n",
        "\n",
        "        model = XGBClassifier(**params)\n",
        "\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_val_proba)\n",
        "        fold_aucs.append(auc)\n",
        "\n",
        "    return float(np.mean(fold_aucs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1676158f1f6a46d58fb81b4d3e70c56f",
            "5ca4b350282a4531810e0778a7c17daa",
            "bc0227ed2bd240e4a146414d0a099dfd",
            "e434730425cd479081fc0eb0308e104e",
            "71c3a69f330147b4a1799c1865e3c8b9",
            "29b6724ff8ca4977a106dff43c52b872",
            "c06c0f32ef7543c4b366006b21eaf8b1",
            "a866238715dc4563a054740b9a746b23",
            "e140587dd6fb410ca3eb7c191ebac579",
            "19900fddcbce4254a32a7facb8a7f21b",
            "a25fb8a9c5884c028309b32ce78c16e1"
          ]
        },
        "id": "IWeIvdNx0XPM",
        "outputId": "e74e7303-dfa9-46b0-c090-4e47932d4c92"
      },
      "outputs": [],
      "source": [
        "db_path = \"/Users/kabdulasset/Desktop/Hackaton/fraud_optuna.db\" \n",
        "storage_url = f\"sqlite:///{db_path}\"\n",
        "\n",
        "print(\"Using storage:\", storage_url)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    study_name=\"fraud_xgb_study\",\n",
        "    direction=\"maximize\",\n",
        "    storage=storage_url,\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "print(\"Best AUC:\", study.best_value)\n",
        "print(\"Best params:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing threshold from PR curve from Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_train_with_scores = full_train_with_scores.reset_index(drop=True)\n",
        "X_train_final          = X_train_final.reset_index(drop=True)\n",
        "y_train_final          = y_train_final.reset_index(drop=True)\n",
        "\n",
        "X = X_train_final\n",
        "y = y_train_final\n",
        "groups = full_train_with_scores[\"cst_dim_id\"].reset_index(drop=True)\n",
        "\n",
        "best_params = study.best_params.copy()\n",
        "\n",
        "# Add fixed params\n",
        "best_params.update({\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": 50,\n",
        "})\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_pred = np.zeros(len(X))\n",
        "\n",
        "for train_idx, val_idx in cv.split(X, y, groups):\n",
        "    X_tr = X.iloc[train_idx]\n",
        "    X_val = X.iloc[val_idx]\n",
        "    y_tr = y.iloc[train_idx]\n",
        "    y_val = y.iloc[val_idx]\n",
        "\n",
        "    # scale_pos_weight per fold\n",
        "    neg = (y_tr == 0).sum()\n",
        "    pos = (y_tr == 1).sum()\n",
        "    best_params[\"scale_pos_weight\"] = neg / max(pos, 1)\n",
        "\n",
        "    model = XGBClassifier(**best_params)\n",
        "    model.fit(X_tr, y_tr, verbose=False)\n",
        "\n",
        "    oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) PR curve from OOF predictions\n",
        "prec_cv, rec_cv, thr_cv = precision_recall_curve(y, oof_pred)\n",
        "\n",
        "prec_thr = prec_cv[1:]  # align with thresholds\n",
        "rec_thr  = rec_cv[1:]\n",
        "\n",
        "f1_thr = 2 * prec_thr * rec_thr / (prec_thr + rec_thr + 1e-9)\n",
        "\n",
        "pr_table = pd.DataFrame({\n",
        "    \"threshold\": thr_cv,\n",
        "    \"precision\": prec_thr,\n",
        "    \"recall\": rec_thr,\n",
        "    \"f1\": f1_thr\n",
        "})\n",
        "\n",
        "# Optional: sort by threshold (ascending)\n",
        "pr_table = pr_table.sort_values(\"threshold\").reset_index(drop=True)\n",
        "\n",
        "# Sort by F1 (highest first)\n",
        "pr_table_top50 = pr_table.sort_values(\"f1\", ascending=False).head(60)\n",
        "\n",
        "print(pr_table_top50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot of PR Curve\n",
        "plt.figure(figsize=(7, 6))\n",
        "\n",
        "scatter = plt.scatter(\n",
        "    rec_thr,\n",
        "    prec_thr,\n",
        "    c=thr_cv,\n",
        "    s=10,\n",
        "    cmap=\"viridis\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision–Recall Curve (colored by threshold)\")\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label(\"Threshold\")\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chosen_threshold = 0.679180 # <- we choose mannually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reLQ3jjt0XPM"
      },
      "source": [
        "## Final XGBoost Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recompute imbalance on full training data\n",
        "neg_full = (y_train_final == 0).sum()\n",
        "pos_full = (y_train_final == 1).sum()\n",
        "\n",
        "final_params = best_params.copy()\n",
        "final_params[\"scale_pos_weight\"] = neg_full / max(pos_full, 1)\n",
        "\n",
        "best_xgb = XGBClassifier(**final_params)\n",
        "best_xgb.fit(X_train_final, y_train_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict probabilities on test set\n",
        "y_proba_test = best_xgb.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "# Threshold-free metrics\n",
        "roc_auc = roc_auc_score(y_test_final, y_proba_test)\n",
        "pr_auc = average_precision_score(y_test, y_proba_test)\n",
        "print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"Test PR-AUC:  {pr_auc:.4f}\")\n",
        "\n",
        "# Apply your manually chosen threshold\n",
        "y_pred_test = (y_proba_test >= chosen_threshold).astype(int)\n",
        "\n",
        "print(\"\\nConfusion matrix (test):\")\n",
        "print(confusion_matrix(y_test_final, y_pred_test))\n",
        "\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(classification_report(y_test_final, y_pred_test, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Extract GAIN importance\n",
        "booster = best_xgb.get_booster()\n",
        "importance_raw = booster.get_score(importance_type=\"gain\")\n",
        "\n",
        "# Use feature names directly (since your model stores real names)\n",
        "importance_dict = importance_raw\n",
        "\n",
        "feat_imp = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": list(importance_dict.keys()),\n",
        "        \"importance\": list(importance_dict.values())\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "topN = 20\n",
        "df_plot = feat_imp.head(topN)   # <-- no reverse, biggest first (top)\n",
        "\n",
        "# 2. Beautiful descending plot\n",
        "plt.figure(figsize=(10, 9))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "ax = sns.barplot(\n",
        "    data=df_plot,\n",
        "    x=\"importance\",\n",
        "    y=\"feature\",\n",
        "    palette=\"viridis\"\n",
        ")\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(df_plot[\"importance\"]):\n",
        "    ax.text(\n",
        "        v * 1.01,\n",
        "        i,\n",
        "        f\"{v:.1f}\",\n",
        "        va=\"center\",\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "plt.title(\"XGBoost Feature Importance (Gain)\", fontsize=18, weight=\"bold\")\n",
        "plt.xlabel(\"Importance (Gain)\", fontsize=14)\n",
        "plt.ylabel(\"Feature\", fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "9mnetZhY0XPM",
        "outputId": "7c918e74-0562-4a7d-bc00-2ed95a9e2671"
      },
      "outputs": [],
      "source": [
        "explainer = shap.TreeExplainer(best_xgb)\n",
        "shap_values = explainer.shap_values(X_test_final)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_test_final)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PR Curve of the Test Set for Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKpebTFk0XPM"
      },
      "outputs": [],
      "source": [
        "prec, rec, thr = precision_recall_curve(y_test, y_proba_test)\n",
        "\n",
        "# thr has length = len(prec) - 1\n",
        "# For each threshold thr[i], the corresponding point on curve is (rec[i+1], prec[i+1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Align with thresholds\n",
        "prec_thr = prec[1:]   # length n\n",
        "rec_thr  = rec[1:]    # length n\n",
        "\n",
        "pr_table = pd.DataFrame({\n",
        "    \"threshold\": thr,       # length n\n",
        "    \"precision\": prec_thr,  # length n\n",
        "    \"recall\": rec_thr,      # length n\n",
        "})\n",
        "\n",
        "\n",
        "# Optional: sort by threshold (ascending)\n",
        "pr_table = pr_table.sort_values(\"threshold\").reset_index(drop=True)\n",
        "\n",
        "# Sort by F1 (highest first)\n",
        "pr_table_top50 = pr_table.sort_values(\"precision\", ascending=False).head(50)\n",
        "\n",
        "print(pr_table_top50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "l1B6Aueb0XPM",
        "outputId": "7e2c7307-310e-4047-81c5-fd72770e578e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Precompute metrics for each threshold to show in the title\n",
        "thresholds = thr\n",
        "points_x = rec[1:]   # recall for each threshold\n",
        "points_y = prec[1:]  # precision for each threshold\n",
        "\n",
        "f1_list = []\n",
        "p_list = []\n",
        "r_list = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_proba_test >= t).astype(int)\n",
        "    p = precision_score(y_test, y_pred_t, zero_division=0)\n",
        "    r = recall_score(y_test, y_pred_t, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred_t, zero_division=0)\n",
        "    p_list.append(p)\n",
        "    r_list.append(r)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "# Base figure: PR curve + initial point\n",
        "fig = go.Figure()\n",
        "\n",
        "# PR curve\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=rec,\n",
        "    y=prec,\n",
        "    mode=\"lines\",\n",
        "    name=\"PR curve\"\n",
        "))\n",
        "\n",
        "# Initial point (use first threshold)\n",
        "init_idx = 0\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[points_x[init_idx]],\n",
        "    y=[points_y[init_idx]],\n",
        "    mode=\"markers\",\n",
        "    marker=dict(size=12),\n",
        "    name=\"Current threshold\"\n",
        "))\n",
        "\n",
        "# Create frames for slider animation\n",
        "frames = []\n",
        "for i, t in enumerate(thresholds):\n",
        "    frames.append(go.Frame(\n",
        "        data=[\n",
        "            # PR curve (unchanged)\n",
        "            go.Scatter(x=rec, y=prec, mode=\"lines\", name=\"PR curve\"),\n",
        "            # Moving point\n",
        "            go.Scatter(\n",
        "                x=[points_x[i]],\n",
        "                y=[points_y[i]],\n",
        "                mode=\"markers\",\n",
        "                marker=dict(size=12),\n",
        "                name=\"Current threshold\"\n",
        "            )\n",
        "        ],\n",
        "        name=f\"{t:.3f}\",\n",
        "        layout=go.Layout(\n",
        "            title=(\n",
        "                f\"Precision–Recall Curve | \"\n",
        "                f\"threshold={t:.3f} | \"\n",
        "                f\"Precision={p_list[i]:.3f}, Recall={r_list[i]:.3f}, F1={f1_list[i]:.3f}\"\n",
        "            )\n",
        "        )\n",
        "    ))\n",
        "\n",
        "fig.frames = frames\n",
        "\n",
        "# Slider definition\n",
        "steps = []\n",
        "for i, t in enumerate(thresholds):\n",
        "    step = dict(\n",
        "        method=\"animate\",\n",
        "        args=[\n",
        "            [f\"{t:.3f}\"],\n",
        "            {\n",
        "                \"mode\": \"immediate\",\n",
        "                \"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                \"transition\": {\"duration\": 0}\n",
        "            }\n",
        "        ],\n",
        "        label=f\"{t:.2f}\",\n",
        "    )\n",
        "    steps.append(step)\n",
        "\n",
        "sliders = [dict(\n",
        "    active=0,\n",
        "    currentvalue={\"prefix\": \"Threshold: \"},\n",
        "    pad={\"t\": 50},\n",
        "    steps=steps\n",
        ")]\n",
        "\n",
        "fig.update_layout(\n",
        "    sliders=sliders,\n",
        "    xaxis_title=\"Recall\",\n",
        "    yaxis_title=\"Precision\",\n",
        "    title=(\n",
        "        f\"Precision–Recall Curve | \"\n",
        "        f\"threshold={thresholds[init_idx]:.3f} | \"\n",
        "        f\"Precision={p_list[init_idx]:.3f}, Recall={r_list[init_idx]:.3f}, F1={f1_list[init_idx]:.3f}\"\n",
        "    ),\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1676158f1f6a46d58fb81b4d3e70c56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ca4b350282a4531810e0778a7c17daa",
              "IPY_MODEL_bc0227ed2bd240e4a146414d0a099dfd",
              "IPY_MODEL_e434730425cd479081fc0eb0308e104e"
            ],
            "layout": "IPY_MODEL_71c3a69f330147b4a1799c1865e3c8b9"
          }
        },
        "19900fddcbce4254a32a7facb8a7f21b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b6724ff8ca4977a106dff43c52b872": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca4b350282a4531810e0778a7c17daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b6724ff8ca4977a106dff43c52b872",
            "placeholder": "​",
            "style": "IPY_MODEL_c06c0f32ef7543c4b366006b21eaf8b1",
            "value": "Best trial: 1365. Best value: 0.966929: 100%"
          }
        },
        "71c3a69f330147b4a1799c1865e3c8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25fb8a9c5884c028309b32ce78c16e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a866238715dc4563a054740b9a746b23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0227ed2bd240e4a146414d0a099dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a866238715dc4563a054740b9a746b23",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e140587dd6fb410ca3eb7c191ebac579",
            "value": 100
          }
        },
        "c06c0f32ef7543c4b366006b21eaf8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e140587dd6fb410ca3eb7c191ebac579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e434730425cd479081fc0eb0308e104e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19900fddcbce4254a32a7facb8a7f21b",
            "placeholder": "​",
            "style": "IPY_MODEL_a25fb8a9c5884c028309b32ce78c16e1",
            "value": " 100/100 [08:55&lt;00:00,  4.78s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
